# -*- coding: utf-8 -*-
"""TrainModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhEunqTOPqEdkufWyKShbtoBGHIDKGwY
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import joblib

# --- LOAD CSV SAFELY ---
df = pd.read_csv("/content/YouTubeData.csv", encoding='ISO-8859-1', on_bad_lines='skip', quoting=3)

# --- CLEAN COLUMN NAMES ---
df.columns = df.columns.str.strip().str.lower()

# --- CONVERT BOOLEAN COLUMNS ---
bool_cols = ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
for col in bool_cols:
    df[col] = df[col].astype(str).str.strip().str.upper()
    df[col] = df[col].map({"TRUE": 1, "FALSE": 0})
df.dropna(subset=bool_cols, inplace=True)

# --- CONVERT NUMERIC COLUMNS SAFELY ---
numeric_cols = ["views", "likes", "dislikes", "comment_count"]
for col in numeric_cols:
    df[col] = df[col].astype(str).str.replace(",", "").str.strip()
    df[col] = pd.to_numeric(df[col], errors="coerce")
df.dropna(subset=numeric_cols, inplace=True)

# --- CLEAN DATE FIELDS ---
df["trending_date"] = pd.to_datetime(df["trending_date"], format="%y.%d.%m", errors="coerce")
df["publish_time"] = pd.to_datetime(df["publish_time"], errors="coerce").dt.tz_localize(None)
df.dropna(subset=["trending_date", "publish_time"], inplace=True)

# --- FEATURE ENGINEERING ---
df["views_log"] = (df["views"] + 1).apply(np.log)
df["likes_log"] = (df["likes"] + 1).apply(np.log)
df["comment_count_log"] = (df["comment_count"] + 1).apply(np.log)

first_trend = df.sort_values("trending_date").groupby("video_id").first().reset_index()
max_views = df.groupby("video_id")["views"].max().reset_index()
max_views.columns = ["video_id", "max_views"]
first_trend = first_trend.merge(max_views, on="video_id")

# Focus on borderline videos
low_view_df = first_trend[(first_trend["views"] < 800000) & (first_trend["views"] > 500000)].copy()

low_view_df["like_ratio"] = low_view_df["likes"] / (low_view_df["likes"] + low_view_df["dislikes"] + 1)
low_view_df["comment_ratio"] = low_view_df["comment_count"] / (low_view_df["views"] + 1)
low_view_df["publish_hour"] = low_view_df["publish_time"].dt.hour
low_view_df["publish_dayofweek"] = low_view_df["publish_time"].dt.dayofweek
low_view_df["days_since_publish"] = (
    low_view_df["trending_date"] - low_view_df["publish_time"].dt.normalize()
).dt.days

# --- TARGET ---
low_view_df["will_be_viral"] = (low_view_df["max_views"] >= 1_000_000).astype(int)

features = [
    "category_id", "comments_disabled", "ratings_disabled",
    "video_error_or_removed", "publish_hour", "publish_dayofweek",
    "like_ratio", "comment_ratio", "days_since_publish",
    "comment_count", "likes"
]
X = low_view_df[features]
y = low_view_df["will_be_viral"]

# One-hot encode category_id as before
X = pd.get_dummies(X, columns=["category_id"], drop_first=True)
X = X.fillna(0)

# Save final column list
feature_columns = X.columns.tolist()

# Train and save
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)
model.fit(X, y)

import joblib
joblib.dump(model, "logistic_model.pkl")
joblib.dump(feature_columns, "feature_columns.pkl")


print("‚úÖ Model training complete. Files saved.")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Predict on test set
y_pred = model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"üîç Model Accuracy: {accuracy:.2%}")

# Classification report (precision, recall, f1)
print("\nüìã Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
print("\nüî¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))